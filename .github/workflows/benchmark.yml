name: Model Quality Benchmarking

on:
  push:
    branches: [main]
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:
    inputs:
      full_benchmark:
        description: 'Run full benchmark suite'
        required: false
        default: 'false'

# Prevent concurrent updates to benchmark data
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Dynamic matrix generation based on available models
  setup-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      should_run: ${{ steps.check-changes.outputs.should_run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Check if benchmark should run
        id: check-changes
        run: |
          # Always run on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For PRs, check if relevant files changed
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Check for changes in models, training, or benchmark files
            changed_files=$(git diff --name-only ${{ github.event.pull_request.base.sha }}...${{ github.sha }})
            
            if echo "$changed_files" | grep -E "(xtylearner/models/|xtylearner/training/|eval\.py|benchmark_config\.json|\.github/workflows/benchmark\.yml)"; then
              echo "should_run=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Generate benchmark matrix
        id: set-matrix
        if: steps.check-changes.outputs.should_run == 'true'
        run: |
          # Create matrix based on PR vs main branch
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Limited matrix for PRs - quick smoke test
            matrix='{"model":["cycle_dual","mean_teacher"],"dataset":["synthetic"]}'
          elif [[ "${{ github.event.inputs.full_benchmark }}" == "true" ]]; then
            # Full matrix for manual dispatch - comprehensive set of diverse models
            matrix='{"model":["cycle_dual","mean_teacher","prob_circuit","ganite","flow_ssc","vat","fixmatch","dragon_net","diffusion_cevae","bridge_diff","cacore","ss_cevae","tab_jepa","masked_tabular_transformer","gnn_scm","joint_ebm","factor_vae_plus","semiite","vacim","multitask"],"dataset":["synthetic","synthetic_mixed"]}'
          else
            # Default matrix for main branch - expanded set for regular testing
            matrix='{"model":["cycle_dual","mean_teacher","prob_circuit","ganite","flow_ssc","vat","fixmatch","dragon_net","diffusion_cevae","bridge_diff","cacore","ss_cevae"],"dataset":["synthetic","synthetic_mixed"]}'
          fi
          
          echo "matrix=$matrix" >> $GITHUB_OUTPUT
          echo "Generated matrix: $matrix"

  # Parallel benchmark execution
  benchmark:
    needs: setup-matrix
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.setup-matrix.outputs.matrix) }}
      max-parallel: 6  # Increased for more models
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python with cache
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: requirements.txt
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
          pip install -e .
      
      # Run benchmarks with retry logic
      - name: Run benchmarks
        env:
          PYTHONUNBUFFERED: 1
        run: |
          set -euo pipefail
          
          echo "Running benchmark for model: ${{ matrix.model }}, dataset: ${{ matrix.dataset }}"
          
          # Run with timeout (extended for complex models)
          timeout 1800 python eval.py \
            --model ${{ matrix.model }} \
            --dataset ${{ matrix.dataset }} \
            --output results-${{ matrix.model }}-${{ matrix.dataset }}.json || {
              echo "Benchmark failed or timed out for ${{ matrix.model }} on ${{ matrix.dataset }}"
              echo '{"results":[],"metadata":{"error":"timeout","model":"${{ matrix.model }}","dataset":"${{ matrix.dataset }}"}}' > results-${{ matrix.model }}-${{ matrix.dataset }}.json
            }
      
      # Upload individual results
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.model }}-${{ matrix.dataset }}
          path: results-${{ matrix.model }}-${{ matrix.dataset }}.json
          retention-days: 7

  # Aggregate and analyze results
  analyze:
    needs: [setup-matrix, benchmark]
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      pages: write
      id-token: write
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install analysis dependencies
        run: |
          python -m pip install --upgrade pip
          pip install matplotlib pandas numpy scipy
          # Set headless backend
          export MPLBACKEND=Agg
      
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: ./results
      
      - name: Create scripts directory
        run: mkdir -p scripts
      
      - name: Create aggregation script
        run: |
          cat > scripts/aggregate_results.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import sys
          import os
          from pathlib import Path
          from datetime import datetime
          import argparse
          
          def aggregate_results(input_dir, history_file, output_file):
              results_dir = Path(input_dir)
              all_results = []
              
              # Collect all result files
              for result_file in results_dir.rglob("*.json"):
                  try:
                      with open(result_file) as f:
                          data = json.load(f)
                          if "results" in data:
                              all_results.extend(data["results"])
                  except Exception as e:
                      print(f"Error reading {result_file}: {e}")
              
              # Load existing history
              history = []
              history_path = Path(history_file)
              if history_path.exists():
                  try:
                      with open(history_path) as f:
                          history = json.load(f)
                  except:
                      history = []
              
              # Create current benchmark entry
              current_entry = {
                  "timestamp": datetime.utcnow().isoformat(),
                  "commit": os.environ.get("GITHUB_SHA", "unknown"),
                  "results": all_results,
                  "metadata": {
                      "total_benchmarks": len(all_results),
                      "models": list(set(r["name"].split("_")[0] for r in all_results)),
                      "datasets": list(set(r["name"].split("_")[1] for r in all_results))
                  }
              }
              
              # Add to history
              history.append(current_entry)
              
              # Keep only last 100 entries
              history = history[-100:]
              
              # Save updated history
              with open(history_path, 'w') as f:
                  json.dump(history, f, indent=2)
              
              # Save current results
              with open(output_file, 'w') as f:
                  json.dump(current_entry, f, indent=2)
              
              print(f"Aggregated {len(all_results)} results")
              print(f"History contains {len(history)} entries")
          
          if __name__ == "__main__":
              import os
              parser = argparse.ArgumentParser()
              parser.add_argument("--input-dir", required=True)
              parser.add_argument("--history-file", required=True)
              parser.add_argument("--output-file", required=True)
              args = parser.parse_args()
              
              aggregate_results(args.input_dir, args.history_file, args.output_file)
          EOF
      
      - name: Aggregate results and update history
        run: |
          python scripts/aggregate_results.py \
            --input-dir ./results \
            --history-file history.json \
            --output-file current-benchmarks.json

      - name: Generate benchmark charts and summary
        if: always()
        run: |
          python scripts/generate_simple_charts.py \
            --history history.json \
            --output-dir ./charts \
            --markdown-output benchmark-summary.md

      - name: Attach benchmark summary to job summary
        if: always()
        run: |
          if [ -f benchmark-summary.md ]; then
            cat benchmark-summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No benchmark results available._" >> "$GITHUB_STEP_SUMMARY"
          fi
      
      # Deploy to GitHub Pages (main branch only)
      - name: Deploy dashboard
        if: github.ref == 'refs/heads/main' && success()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./charts
          
      - name: Upload benchmark history
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-history
          path: history.json
          retention-days: 90