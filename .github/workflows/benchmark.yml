# yamllint disable rule:truthy
---
name: Benchmark Models

on:
  pull_request:
    types: [opened, synchronize]
  push:
    branches: [main]

# yamllint enable rule:truthy

jobs:
  detect-changes:
    if: github.actor != 'github-actions[bot]'
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.models.outputs.should_run }}
      model_chunks: ${{ steps.models.outputs.model_chunks }}
      models: ${{ steps.models.outputs.models }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}
      - name: Fetch base reference
        if: github.event_name == 'pull_request'
        run: git fetch origin ${{ github.base_ref }} --depth=1
      - name: Determine benchmark models
        id: models
        env:
          DEFAULT_MODELS: cycle_dual
          CHUNK_SIZE: '2'
        run: |
          python <<'PY'
          import json
          import os
          import subprocess
          from pathlib import Path

          def has_ref(ref: str) -> bool:
              return subprocess.run(
                  ["git", "rev-parse", "--verify", ref],
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL,
                  check=False,
              ).returncode == 0

          base_ref = os.environ.get("GITHUB_BASE_REF")
          compare_ref = f"origin/{base_ref}" if base_ref else "origin/main"
          if not has_ref(compare_ref):
              compare_ref = "HEAD^"

          diff_args = ["git", "diff", "--name-only"]
          if has_ref(compare_ref):
              diff_args.append(f"{compare_ref}...HEAD")

          result = subprocess.run(
              diff_args,
              check=False,
              stdout=subprocess.PIPE,
              text=True,
          )

          changed_files: list[str] = []
          for line in result.stdout.splitlines():
              line = line.strip()
              if line:
                  changed_files.append(line)

          changed_models = set()
          for path in changed_files:
              if (
                  not path.startswith("xtylearner/models/")
                  or not path.endswith(".py")
              ):
                  continue
              name = Path(path).stem
              if name.endswith("_model"):
                  name = name[: -len("_model")]
              changed_models.add(name)

          default_models = [
              model.strip()
              for model in os.environ.get("DEFAULT_MODELS", "").split(",")
              if model.strip()
          ]
          if changed_models:
              selected = sorted(set(default_models) | changed_models)
          else:
              selected = default_models

          if not selected:
              selected = ["cycle_dual"]

          chunk_size = max(1, int(os.environ.get("CHUNK_SIZE", "1")))
          chunks: list[list[str]] = []
          for index in range(0, len(selected), chunk_size):
              chunks.append(selected[index : index + chunk_size])

          output_path = Path(os.environ["GITHUB_OUTPUT"])
          chunk_output = ["should_run=true", f"models={','.join(selected)}"]
          chunk_strings = [",".join(chunk) for chunk in chunks]
          chunk_output.append("model_chunks=" + json.dumps(chunk_strings))
          output_text = "\n".join(chunk_output) + "\n"
          output_path.write_text(output_text, encoding="utf-8")

          print("Selected models:", ", ".join(selected))
          for index, chunk in enumerate(chunks):
              print(f"Chunk {index}: {', '.join(chunk)}")
          PY

  benchmark:
    needs: detect-changes
    if: needs.detect-changes.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    env:
      PYTHON_VERSION: '3.12'
    strategy:
      fail-fast: false
      matrix:
        model_chunk: >-
          ${{ fromJson(needs.detect-changes.outputs.model_chunks) }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}
      - name: Ensure main branch is available
        run: |
          set -euo pipefail
          git fetch origin main --depth=50
          current_branch=$(git rev-parse --abbrev-ref HEAD)
          if [ "$current_branch" = "main" ]; then
            exit 0
          fi
          if git show-ref --verify --quiet refs/heads/main; then
            git branch --force main origin/main
          else
            git branch main origin/main
          fi
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'  # yamllint disable-line rule:truthy
          cache-dependency-path: |
            asv.conf.json
            requirements.txt
      - name: Install ASV tooling
        run: |
          python -m pip install --upgrade pip
          python -m pip install asv
      - name: Compute ASV cache key
        id: asv-cache
        run: |
          python <<'PY'
          import hashlib
          import os
          from pathlib import Path

          paths = [Path('asv.conf.json'), Path('requirements.txt')]
          digest = hashlib.sha256()
          for path in paths:
              if path.exists():
                  digest.update(path.read_bytes())

          fingerprint = digest.hexdigest()
          prefix = 'asv-env-ubuntu-latest-py3.12'

          output = Path(os.environ['GITHUB_OUTPUT'])
          with output.open('a', encoding='utf-8') as handle:
              handle.write(f"fingerprint={fingerprint}\n")
              handle.write(f"prefix={prefix}\n")
              handle.write(f"cache_key={prefix}-{fingerprint}\n")
          PY
      - name: Restore cached ASV environments
        id: cache-asv
        uses: actions/cache@v4
        with:
          path: .asv/env
          key: ${{ steps.asv-cache.outputs.cache_key }}
          restore-keys: |
            ${{ steps.asv-cache.outputs.prefix }}-
      - name: Run ASV benchmarks
        env:
          BENCHMARK_MODELS: ${{ matrix.model_chunk }}
        run: |
          set -euo pipefail
          OS_NAME=$(python - <<'PY'
          import platform

          system = platform.system() or "UnknownOS"
          release = platform.release() or ""
          print((system + " " + release).strip())
          PY
          )

          ARCH=$(uname -m)

          CPU_INFO=$(python - <<'PY'
          import platform
          import subprocess

          cpu = platform.processor()
          if not cpu or cpu.lower() in {"", "x86_64", "amd64"}:
              try:
                  output = subprocess.check_output(["lscpu"], text=True)
              except Exception:
                  cpu = "Unknown CPU"
              else:
                  model_line = None
                  for line in output.splitlines():
                      if line.lower().startswith("model name:"):
                          model_line = line
                          break
                  if model_line:
                      cpu = model_line.split(":", 1)[1].strip()
                  else:
                      cpu = (
                          output.strip().splitlines()[0].strip()
                          or "Unknown CPU"
                      )
          print(cpu)
          PY
          )

          NUM_CPU=$(python - <<'PY'
          import os

          print(os.cpu_count() or 1)
          PY
          )

          RAM=$(python - <<'PY'
          from pathlib import Path

          meminfo = Path("/proc/meminfo")
          total = None
          if meminfo.exists():
              for line in meminfo.read_text().splitlines():
                  if line.startswith("MemTotal:"):
                      parts = line.split()
                      if len(parts) >= 2:
                          try:
                              total = int(parts[1]) * 1024
                          except ValueError:
                              total = None
                      break

          if total:
              gib = total / (1024 ** 3)
              if gib >= 1:
                  value = f"{gib:.1f}GB"
              else:
                  mib = total / (1024 ** 2)
                  value = f"{mib:.0f}MB"
          else:
              value = "Unknown RAM"
          print(value)
          PY
          )

          asv machine \
            --machine github-actions \
            --os "$OS_NAME" \
            --arch "$ARCH" \
            --cpu "$CPU_INFO" \
            --num_cpu "$NUM_CPU" \
            --ram "$RAM" \
            --yes
          COMMIT_HASH=$(git rev-parse HEAD)
          if git rev-parse "${COMMIT_HASH}^" >/dev/null 2>&1; then
            COMMIT_SPEC="${COMMIT_HASH}^!"
          else
            COMMIT_SPEC="$COMMIT_HASH"
          fi
          echo "Running benchmarks for commit: $COMMIT_HASH"
          echo "Model chunk: ${BENCHMARK_MODELS:-<default>}"
          asv run "$COMMIT_SPEC" \
            --machine github-actions \
            --config asv.conf.json \
            --show-stderr
          if ! find .asv/results/github-actions -maxdepth 1 \
              -name "*.json" -not -name "machine.json" -print -quit; then
            echo "No benchmark result files were produced"
            exit 1
          fi
          asv publish --config asv.conf.json --html-dir .asv/html
      - name: Prepare artifact name
        id: artifact
        env:
          MODEL_CHUNK: ${{ matrix.model_chunk }}
        run: |
          SAFE_NAME=${MODEL_CHUNK:-default}
          SAFE_NAME=$(printf '%s' "$SAFE_NAME" | tr ' ,/' '---')
          SAFE_NAME=$(printf '%s' "$SAFE_NAME" | tr -c 'A-Za-z0-9-_' '_')
          echo "name=asv-results-$SAFE_NAME" >> "$GITHUB_OUTPUT"
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.artifact.outputs.name }}
          path: |
            .asv/results/github-actions
            .asv/html

  consolidate-results:
    needs: [detect-changes, benchmark]
    if: needs.detect-changes.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}
      - name: Ensure main branch exists
        run: |
          # Ensure we have the main branch for ASV publish
          if ! git show-ref --verify --quiet refs/heads/main; then
            if git show-ref --verify --quiet refs/remotes/origin/main; then
              git branch main origin/main
            else
              git fetch origin main:main
            fi
          fi
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: |
            asv.conf.json
            requirements.txt
      - name: Install ASV tooling
        run: |
          python -m pip install --upgrade pip
          python -m pip install asv
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: asv-results-*
          path: artifact-downloads
          merge-multiple: true
      - name: Consolidate benchmark results
        run: |
          set -euo pipefail
          echo "=== Consolidating benchmark results ==="
          
          # Install jq for JSON validation
          sudo apt-get update && sudo apt-get install -y jq
          
          # Debug: Show what we downloaded
          echo "=== Downloaded artifact structure ==="
          find artifact-downloads -type f | head -20
          echo "=== Looking for results directories ==="
          find artifact-downloads -type d -name "results" | head -10
          echo "=== Looking for JSON files ==="
          find artifact-downloads -name "*.json" | head -10
          
          # Create consolidated results directory
          mkdir -p .asv/results/github-actions
          
          # Copy machine.json from any artifact
          find artifact-downloads -name "machine.json" -exec cp {} .asv/results/github-actions/ \; -quit
          
          # Merge all benchmark result files (only from results directory, not HTML)
          echo "=== Available result files ==="
          find artifact-downloads -path "*/results/github-actions/*.json" -not -name "machine.json" | sort
          
          # Copy all benchmark JSON files with validation
          result_count=0
          model_count=0
          
          # Process only actual benchmark result files (not HTML JSON files)
          while IFS= read -r -d '' file; do
            if [ -f "$file" ] && jq empty "$file" 2>/dev/null; then
              filename=$(basename "$file")
              echo "Processing valid result file: $filename"
              
              # Count models in this file - check if results key exists
              if jq -e '.results' "$file" >/dev/null 2>&1; then
                file_models=$(jq -r '.results | keys | length' "$file" 2>/dev/null || echo "0")
                echo "  Models in file: $file_models"
                model_count=$((model_count + file_models))
                
                cp "$file" ".asv/results/github-actions/$filename"
                result_count=$((result_count + 1))
              else
                echo "  Skipping non-benchmark file: $filename (no .results key)"
              fi
            else
              echo "WARNING: Skipping malformed JSON file: $(basename "$file")"
            fi
          done < <(find artifact-downloads -path "*/results/github-actions/*.json" -not -name "machine.json" -print0)
          
          echo "=== Consolidation Summary ==="
          echo "Total valid result files: $result_count"
          echo "Total models benchmarked: $model_count"
          ls -la .asv/results/github-actions/
          
          # Validate we have results
          if [ "$result_count" -eq 0 ]; then
            echo "ERROR: No valid benchmark results found"
            exit 1
          fi
          
          # Validate we have expected models
          expected_models="${{ needs.detect-changes.outputs.models }}"
          if [ -n "$expected_models" ]; then
            expected_count=$(echo "$expected_models" | tr ',' '\n' | wc -l)
            echo "Expected models: $expected_count, Found models: $model_count"
            if [ "$model_count" -lt "$expected_count" ]; then
              echo "WARNING: Found fewer models than expected"
            fi
          fi
          
          # Generate unified HTML
          echo "=== Generating unified HTML ==="
          echo "Current git branches:"
          git branch -a || echo "Git branch listing failed"
          echo "Current commit:"
          git rev-parse HEAD || echo "Git rev-parse failed"
          
          # Create a temporary config that doesn't require specific branches
          cp asv.conf.json asv-publish.conf.json
          # Get current branch name
          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
          echo "Current branch: $CURRENT_BRANCH"
          
          # Update config to use current branch instead of main
          jq --arg branch "$CURRENT_BRANCH" '.branches = [$branch]' asv.conf.json > asv-publish.conf.json
          
          # Try publish with modified config
          asv publish --config asv-publish.conf.json --html-dir .asv/html --no-pull
          
          # Validate HTML generation
          if [ ! -f ".asv/html/index.html" ]; then
            echo "ERROR: HTML generation failed"
            exit 1
          fi
          
          # Validate HTML content
          if grep -q "No benchmark data available" ".asv/html/index.html"; then
            echo "ERROR: HTML shows no benchmark data"
            exit 1
          fi
          
          echo "HTML generation successful"
          echo "Generated files:"
          find .asv/html -name "*.html" | head -10
      - name: Upload consolidated artifacts
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-asv-results
          path: |
            .asv/results/github-actions
            .asv/html

  deploy-to-pages:
    needs: [detect-changes, consolidate-results]
    if: needs.detect-changes.outputs.should_run == 'true' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: Download consolidated results
        uses: actions/download-artifact@v4
        with:
          name: consolidated-asv-results
          path: asv-consolidated
      - name: Validate deployment package
        run: |
          set -euo pipefail
          echo "=== Validating deployment package ==="
          
          # Check required files exist
          if [ ! -f "asv-consolidated/.asv/html/index.html" ]; then
            echo "ERROR: Missing index.html"
            exit 1
          fi
          
          # Check for required ASV assets
          required_files=("asv.js" "asv.css" "asv_ui.js")
          for file in "${required_files[@]}"; do
            if [ ! -f "asv-consolidated/.asv/html/$file" ]; then
              echo "ERROR: Missing required ASV file: $file"
              exit 1
            fi
          done
          
          # Validate index.html doesn't contain error messages
          if grep -qi "error\|no.*data.*available" "asv-consolidated/.asv/html/index.html"; then
            echo "ERROR: HTML contains error indicators"
            cat asv-consolidated/.asv/html/index.html
            exit 1
          fi
          
          echo "Deployment package validation successful"
          echo "Package contents:"
          find asv-consolidated/.asv/html -type f | head -20
      - name: Setup Pages
        uses: actions/configure-pages@v4
      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: asv-consolidated/.asv/html
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
      - name: Verify deployment
        if: always()
        run: |
          echo "=== Deployment Summary ==="
          echo "Deployment status: ${{ steps.deployment.outcome }}"
          if [ "${{ steps.deployment.outcome }}" = "success" ]; then
            echo "✅ ASV benchmarks successfully deployed to GitHub Pages"
            echo "🔗 URL: ${{ steps.deployment.outputs.page_url }}"
          else
            echo "❌ Deployment failed"
            echo "Check the deployment logs above for details"
          fi
