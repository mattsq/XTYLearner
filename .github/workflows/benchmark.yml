name: Model Quality Benchmarking

on:
  push:
    branches: [main]
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:
    inputs:
      full_benchmark:
        description: 'Run full benchmark suite'
        required: false
        default: 'false'

# Prevent concurrent updates to benchmark data
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Dynamic matrix generation based on available models
  setup-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      should_run: ${{ steps.check-changes.outputs.should_run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Check if benchmark should run
        id: check-changes
        run: |
          # Always run on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For PRs, check if relevant files changed
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Check for changes in models, training, or benchmark files
            changed_files=$(git diff --name-only ${{ github.event.pull_request.base.sha }}...${{ github.sha }})
            
            if echo "$changed_files" | grep -E "(xtylearner/models/|xtylearner/training/|eval\.py|benchmark_config\.json|\.github/workflows/benchmark\.yml)"; then
              echo "should_run=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Generate benchmark matrix
        id: set-matrix
        if: steps.check-changes.outputs.should_run == 'true'
        run: |
          # Create matrix based on PR vs main branch
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Limited matrix for PRs - quick smoke test
            matrix='{"model":["cycle_dual","mean_teacher"],"dataset":["synthetic"]}'
          elif [[ "${{ github.event.inputs.full_benchmark }}" == "true" ]]; then
            # Full matrix for manual dispatch - comprehensive set of diverse models
            matrix='{"model":["cycle_dual","mean_teacher","prob_circuit","ganite","flow_ssc","vat","fixmatch","dragon_net","diffusion_cevae","bridge_diff","cacore","ss_cevae","tab_jepa","masked_tabular_transformer","gnn_scm","joint_ebm","factor_vae_plus","semiite","vacim","multitask"],"dataset":["synthetic","synthetic_mixed"]}'
          else
            # Default matrix for main branch - expanded set for regular testing
            matrix='{"model":["cycle_dual","mean_teacher","prob_circuit","ganite","flow_ssc","vat","fixmatch","dragon_net","diffusion_cevae","bridge_diff","cacore","ss_cevae"],"dataset":["synthetic","synthetic_mixed"]}'
          fi
          
          echo "matrix=$matrix" >> $GITHUB_OUTPUT
          echo "Generated matrix: $matrix"

  # Parallel benchmark execution
  benchmark:
    needs: setup-matrix
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.setup-matrix.outputs.matrix) }}
      max-parallel: 6  # Increased for more models
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python with cache
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: requirements.txt
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
          pip install -e .
      
      # Run benchmarks with retry logic
      - name: Run benchmarks
        env:
          PYTHONUNBUFFERED: 1
        run: |
          set -euo pipefail
          
          echo "Running benchmark for model: ${{ matrix.model }}, dataset: ${{ matrix.dataset }}"
          
          # Run with timeout (extended for complex models)
          timeout 1800 python eval.py \
            --model ${{ matrix.model }} \
            --dataset ${{ matrix.dataset }} \
            --output results-${{ matrix.model }}-${{ matrix.dataset }}.json || {
              echo "Benchmark failed or timed out for ${{ matrix.model }} on ${{ matrix.dataset }}"
              echo '{"results":[],"metadata":{"error":"timeout","model":"${{ matrix.model }}","dataset":"${{ matrix.dataset }}"}}' > results-${{ matrix.model }}-${{ matrix.dataset }}.json
            }
      
      # Upload individual results
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.model }}-${{ matrix.dataset }}
          path: results-${{ matrix.model }}-${{ matrix.dataset }}.json
          retention-days: 7

  # Aggregate and analyze results
  analyze:
    needs: [setup-matrix, benchmark]
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      pages: write
      id-token: write
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install analysis dependencies
        run: |
          python -m pip install --upgrade pip
          pip install matplotlib pandas numpy scipy
          # Set headless backend
          export MPLBACKEND=Agg
      
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: ./results
      
      - name: Create scripts directory
        run: mkdir -p scripts
      
      - name: Create aggregation script
        run: |
          cat > scripts/aggregate_results.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import sys
          import os
          from pathlib import Path
          from datetime import datetime
          import argparse
          
          def aggregate_results(input_dir, history_file, output_file):
              results_dir = Path(input_dir)
              all_results = []
              
              # Collect all result files
              for result_file in results_dir.rglob("*.json"):
                  try:
                      with open(result_file) as f:
                          data = json.load(f)
                          if "results" in data:
                              all_results.extend(data["results"])
                  except Exception as e:
                      print(f"Error reading {result_file}: {e}")
              
              # Load existing history
              history = []
              history_path = Path(history_file)
              if history_path.exists():
                  try:
                      with open(history_path) as f:
                          history = json.load(f)
                  except:
                      history = []
              
              # Create current benchmark entry
              current_entry = {
                  "timestamp": datetime.utcnow().isoformat(),
                  "commit": os.environ.get("GITHUB_SHA", "unknown"),
                  "results": all_results,
                  "metadata": {
                      "total_benchmarks": len(all_results),
                      "models": list(set(r["name"].split("_")[0] for r in all_results)),
                      "datasets": list(set(r["name"].split("_")[1] for r in all_results))
                  }
              }
              
              # Add to history
              history.append(current_entry)
              
              # Keep only last 100 entries
              history = history[-100:]
              
              # Save updated history
              with open(history_path, 'w') as f:
                  json.dump(history, f, indent=2)
              
              # Save current results
              with open(output_file, 'w') as f:
                  json.dump(current_entry, f, indent=2)
              
              print(f"Aggregated {len(all_results)} results")
              print(f"History contains {len(history)} entries")
          
          if __name__ == "__main__":
              import os
              parser = argparse.ArgumentParser()
              parser.add_argument("--input-dir", required=True)
              parser.add_argument("--history-file", required=True)
              parser.add_argument("--output-file", required=True)
              args = parser.parse_args()
              
              aggregate_results(args.input_dir, args.history_file, args.output_file)
          EOF
      
      - name: Aggregate results and update history
        run: |
          python scripts/aggregate_results.py \
            --input-dir ./results \
            --history-file history.json \
            --output-file current-benchmarks.json

      - name: Render markdown summary
        if: always()
        run: |
          if [ -f current-benchmarks.json ]; then
            python scripts/render_benchmark_summary.py \
              --input current-benchmarks.json \
              --output benchmark-summary.md
          else
            echo "_No benchmark results available._" > benchmark-summary.md
          fi
      
      - name: Create simple visualization script
        run: |
          cat > scripts/generate_simple_charts.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import matplotlib
          matplotlib.use('Agg')
          import matplotlib.pyplot as plt
          import pandas as pd
          import numpy as np
          from pathlib import Path
          import argparse
          
          def generate_charts(history_file, output_dir):
              output_path = Path(output_dir)
              output_path.mkdir(exist_ok=True)
              
              # Load history
              with open(history_file) as f:
                  history = json.load(f)
              
              if not history:
                  print("No history data found")
                  return
              
              # Extract latest results for summary
              latest = history[-1]
              
              # Create summary plot
              fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
              
              # Plot 1: RMSE by model
              rmse_data = {}
              accuracy_data = {}
              
              for result in latest["results"]:
                  if "val_outcome_rmse" in result["name"]:
                      model = result["name"].split("_")[0]
                      rmse_data[model] = result["value"]
                  elif "val_treatment_accuracy" in result["name"]:
                      model = result["name"].split("_")[0]
                      accuracy_data[model] = result["value"]
              
              if rmse_data:
                  models = list(rmse_data.keys())
                  values = list(rmse_data.values())
                  ax1.bar(models, values)
                  ax1.set_title('Model RMSE (Lower is Better)')
                  ax1.set_ylabel('RMSE')
                  ax1.tick_params(axis='x', rotation=45)
              
              if accuracy_data:
                  models = list(accuracy_data.keys())
                  values = list(accuracy_data.values())
                  ax2.bar(models, values)
                  ax2.set_title('Model Treatment Accuracy (Higher is Better)')
                  ax2.set_ylabel('Accuracy')
                  ax2.tick_params(axis='x', rotation=45)
              
              plt.tight_layout()
              plt.savefig(output_path / 'benchmark_summary.png', dpi=150, bbox_inches='tight')
              plt.close()
              
              # Generate simple HTML dashboard
              html_content = f"""
              <!DOCTYPE html>
              <html>
              <head>
                  <title>XTYLearner Benchmark Dashboard</title>
                  <style>
                      body {{ font-family: Arial, sans-serif; margin: 40px; }}
                      .header {{ border-bottom: 2px solid #333; padding-bottom: 20px; margin-bottom: 30px; }}
                      .summary {{ background: #f5f5f5; padding: 20px; border-radius: 5px; margin-bottom: 30px; }}
                      .metric {{ display: inline-block; margin: 10px 20px; }}
                      .metric-value {{ font-size: 1.5em; font-weight: bold; color: #2c3e50; }}
                      .metric-name {{ color: #7f8c8d; }}
                      .chart {{ text-align: center; margin: 30px 0; }}
                      table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                      th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                      th {{ background-color: #f2f2f2; }}
                  </style>
              </head>
              <body>
                  <div class="header">
                      <h1>ðŸš€ XTYLearner Model Benchmarks</h1>
                      <p>Last updated: {latest["timestamp"]} | Commit: {latest["commit"][:8]}</p>
                  </div>
                  
                  <div class="summary">
                      <h2>ðŸ“Š Summary</h2>
                      <div class="metric">
                          <div class="metric-value">{len(latest["results"])}</div>
                          <div class="metric-name">Total Benchmarks</div>
                      </div>
                      <div class="metric">
                          <div class="metric-value">{len(latest["metadata"]["models"])}</div>
                          <div class="metric-name">Models Tested</div>
                      </div>
                      <div class="metric">
                          <div class="metric-value">{len(latest["metadata"]["datasets"])}</div>
                          <div class="metric-name">Datasets Used</div>
                      </div>
                  </div>
                  
                  <div class="chart">
                      <h2>ðŸ“ˆ Performance Overview</h2>
                      <img src="benchmark_summary.png" alt="Benchmark Summary" style="max-width: 100%; height: auto;">
                  </div>
                  
                  <h2>ðŸ“‹ Detailed Results</h2>
                  <table>
                      <tr>
                          <th>Model</th>
                          <th>Dataset</th>
                          <th>Metric</th>
                          <th>Value</th>
                          <th>Unit</th>
                          <th>Confidence Interval</th>
                      </tr>
              """
              
              for result in sorted(latest["results"], key=lambda x: x["name"]):
                  parts = result["name"].split("_", 2)
                  model = parts[0] if len(parts) > 0 else "unknown"
                  dataset = parts[1] if len(parts) > 1 else "unknown"
                  metric = parts[2] if len(parts) > 2 else "unknown"
                  
                  ci_range = f"[{result['range'][0]:.3f}, {result['range'][1]:.3f}]"
                  
                  html_content += f"""
                      <tr>
                          <td>{model}</td>
                          <td>{dataset}</td>
                          <td>{metric}</td>
                          <td>{result['value']:.4f}</td>
                          <td>{result['unit']}</td>
                          <td>{ci_range}</td>
                      </tr>
                  """
              
              html_content += """
                  </table>
                  
                  <div style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd; color: #666;">
                      <p>Generated by XTYLearner Benchmark Suite</p>
                  </div>
              </body>
              </html>
              """
              
              with open(output_path / 'index.html', 'w', encoding='utf-8') as f:
                  f.write(html_content)
              
              print(f"Charts generated in {output_path}")
          
          if __name__ == "__main__":
              parser = argparse.ArgumentParser()
              parser.add_argument("--history", required=True)
              parser.add_argument("--output-dir", required=True)
              args = parser.parse_args()
              
              generate_charts(args.history, args.output_dir)
          EOF
      
      - name: Generate benchmark charts
        if: always()
        run: |
          python scripts/generate_simple_charts.py \
            --history history.json \
            --output-dir ./charts

      - name: Attach benchmark summary to job summary
        if: always()
        run: |
          if [ -f benchmark-summary.md ]; then
            cat benchmark-summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No benchmark results available._" >> "$GITHUB_STEP_SUMMARY"
          fi
      
      # Deploy to GitHub Pages (main branch only)
      - name: Deploy dashboard
        if: github.ref == 'refs/heads/main' && success()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./charts
          
      - name: Upload benchmark history
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-history
          path: history.json
          retention-days: 90