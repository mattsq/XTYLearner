name: Model Quality Benchmarking

on:
  push:
    branches: [main]
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:
    inputs:
      full_benchmark:
        description: 'Run full benchmark suite'
        required: false
        default: 'false'

# Prevent concurrent updates to benchmark data
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Dynamic matrix generation based on available models
  setup-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      should_run: ${{ steps.check-changes.outputs.should_run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Check if benchmark should run
        id: check-changes
        run: |
          # Always run on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For PRs, check if relevant files changed
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Check for changes in models, training, or benchmark files
            changed_files=$(git diff --name-only ${{ github.event.pull_request.base.sha }}...${{ github.sha }})
            
            if echo "$changed_files" | grep -E "(xtylearner/models/|xtylearner/training/|eval\.py|benchmark_config\.json|\.github/workflows/benchmark\.yml)"; then
              echo "should_run=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Generate benchmark matrix
        id: set-matrix
        if: steps.check-changes.outputs.should_run == 'true'
        run: |
          # Create matrix based on PR vs main branch
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Limited matrix for PRs - quick smoke test
            matrix='{"model":["cycle_dual","mean_teacher"],"dataset":["synthetic"]}'
          elif [[ "${{ github.event.inputs.full_benchmark }}" == "true" ]]; then
            # Full matrix for manual dispatch - comprehensive set of diverse models
            matrix='{"model":["cycle_dual","mean_teacher","prob_circuit","ganite","flow_ssc","vat","fixmatch","dragon_net","diffusion_cevae","bridge_diff","cacore","ss_cevae","tab_jepa","masked_tabular_transformer","gnn_scm","joint_ebm","factor_vae_plus","semiite","vacim","multitask"],"dataset":["synthetic","synthetic_mixed"]}'
          else
            # Default matrix for main branch - expanded set for regular testing
            matrix='{"model":["cycle_dual","mean_teacher","prob_circuit","ganite","flow_ssc","vat","fixmatch","dragon_net","diffusion_cevae","bridge_diff","cacore","ss_cevae"],"dataset":["synthetic","synthetic_mixed"]}'
          fi
          
          echo "matrix=$matrix" >> $GITHUB_OUTPUT
          echo "Generated matrix: $matrix"

  prepare-runtime:
    needs: setup-matrix
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python with cache
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml
            uv.lock

      - name: Restore pip and torch caches
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/torch
          key: ${{ runner.os }}-deps-${{ hashFiles('requirements.txt', 'pyproject.toml', 'uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-deps-

      - name: Restore prepared virtualenv
        id: venv-cache
        uses: actions/cache@v4
        with:
          path: .venv
          key: ${{ runner.os }}-benchmark-venv-${{ hashFiles('requirements.txt', 'pyproject.toml', 'uv.lock', '.github/workflows/benchmark.yml') }}

      - name: Build virtualenv
        if: steps.venv-cache.outputs.cache-hit != 'true'
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
          pip install matplotlib pandas numpy scipy
          pip install -e .

      - name: Package prepared environment
        run: |
          tar -czf prepared-venv.tar.gz .venv

      - name: Upload prepared environment
        uses: actions/upload-artifact@v4
        with:
          name: prepared-venv
          path: prepared-venv.tar.gz
          retention-days: 3

  # Parallel benchmark execution
  benchmark:
    needs: [setup-matrix, prepare-runtime]
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.setup-matrix.outputs.matrix) }}
      max-parallel: 6  # Increased for more models
    
    steps:
      - uses: actions/checkout@v4

      - name: Download prepared environment
        uses: actions/download-artifact@v4
        with:
          name: prepared-venv
          path: .

      - name: Extract environment
        id: extract-env
        run: |
          SECONDS=0
          tar -xzf prepared-venv.tar.gz
          echo "seconds=$SECONDS" >> $GITHUB_OUTPUT

      - name: Ensure dataset cache directory
        run: mkdir -p .cache/xtylearner

      - name: Restore dataset cache
        uses: actions/cache@v4
        with:
          path: .cache/xtylearner
          key: ${{ runner.os }}-dataset-${{ matrix.dataset }}-${{ hashFiles('eval.py') }}
          restore-keys: |
            ${{ runner.os }}-dataset-${{ matrix.dataset }}-

      # Run benchmarks with retry logic
      - name: Run benchmarks
        id: run-benchmarks
        env:
          PYTHONUNBUFFERED: 1
        run: |
          set -euo pipefail
          SECONDS=0

          echo "Running benchmark for model: ${{ matrix.model }}, dataset: ${{ matrix.dataset }}"

          if ! timeout 1800 ./.venv/bin/python eval.py \
            --model ${{ matrix.model }} \
            --dataset ${{ matrix.dataset }} \
            --output results-${{ matrix.model }}-${{ matrix.dataset }}.json; then
            echo "Benchmark failed or timed out for ${{ matrix.model }} on ${{ matrix.dataset }}"
            cat <<'EOF' > results-${{ matrix.model }}-${{ matrix.dataset }}.json
{"results":[],"metadata":{"error":"timeout","model":"${{ matrix.model }}","dataset":"${{ matrix.dataset }}"}}
EOF
          fi

          echo "seconds=$SECONDS" >> $GITHUB_OUTPUT

      - name: Persist timing metadata
        env:
          MODEL: ${{ matrix.model }}
          DATASET: ${{ matrix.dataset }}
          ENV_SETUP_SECONDS: ${{ steps.extract-env.outputs.seconds }}
          BENCHMARK_SECONDS: ${{ steps.run-benchmarks.outputs.seconds }}
        run: |
          ./.venv/bin/python - <<'PY'
import json
import os
from pathlib import Path

model = os.environ["MODEL"]
dataset = os.environ["DATASET"]
env_seconds = float(os.environ.get("ENV_SETUP_SECONDS", "0") or 0.0)
benchmark_seconds = float(os.environ.get("BENCHMARK_SECONDS", "0") or 0.0)

result_path = Path(f"results-{model}-{dataset}.json")
timings = []
if result_path.exists():
    with result_path.open() as fh:
        try:
            payload = json.load(fh)
        except json.JSONDecodeError:
            payload = {}
        if isinstance(payload, dict):
            metadata = payload.get("metadata", {})
            if isinstance(metadata, dict):
                timings = metadata.get("timings", [])

timing_record = {}
if isinstance(timings, list):
    for candidate in timings:
        if isinstance(candidate, dict) and candidate.get("model") == model and candidate.get("dataset") == dataset:
            timing_record.update(candidate)
            break
    else:
        if timings and isinstance(timings[0], dict):
            timing_record.update(timings[0])
timing_record.setdefault("model", model)
timing_record.setdefault("dataset", dataset)
timing_record["environment_setup_seconds"] = env_seconds
timing_record["execution_step_seconds"] = benchmark_seconds

with Path(f"timing-{model}-{dataset}.json").open("w") as fh:
    json.dump(timing_record, fh, indent=2)
PY

      # Upload individual results
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.model }}-${{ matrix.dataset }}
          path: |
            results-${{ matrix.model }}-${{ matrix.dataset }}.json
            timing-${{ matrix.model }}-${{ matrix.dataset }}.json
          retention-days: 7

  # Aggregate and analyze results
  analyze:
    needs: [setup-matrix, benchmark]
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      pages: write
      id-token: write
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Download prepared environment
        uses: actions/download-artifact@v4
        with:
          name: prepared-venv
          path: .

      - name: Extract environment
        run: |
          tar -xzf prepared-venv.tar.gz

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: ./results

      - name: Aggregate results and update history
        run: |
          ./.venv/bin/python scripts/aggregate_results.py \
            --input-dir ./results \
            --history-file history.json \
            --output-file current-benchmarks.json

      - name: Generate benchmark charts and summary
        if: always()
        env:
          MPLBACKEND: Agg
        run: |
          ./.venv/bin/python scripts/generate_simple_charts.py \
            --history history.json \
            --output-dir ./charts \
            --markdown-output benchmark-summary.md

      - name: Summarize benchmark timings
        if: always()
        run: |
          ./.venv/bin/python scripts/summarize_timings.py \
            --current current-benchmarks.json \
            --timing-dir ./results \
            --summary-out timing-summary.md

      - name: Attach summaries to job summary
        if: always()
        run: |
          if [ -f benchmark-summary.md ]; then
            cat benchmark-summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No benchmark results available._" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -f timing-summary.md ]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            cat timing-summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No timing data captured._" >> "$GITHUB_STEP_SUMMARY"
          fi
      
      # Deploy to GitHub Pages (main branch only)
      - name: Deploy dashboard
        if: github.ref == 'refs/heads/main' && success()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./charts
          
      - name: Upload benchmark history
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-history
          path: history.json
          retention-days: 90
