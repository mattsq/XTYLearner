---
name: Model Quality Benchmarking

# yamllint disable rule:truthy
on:
  push:
    branches: [main]
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:
    inputs:
      full_benchmark:
        description: 'Run full benchmark suite'
        required: false
        default: false
# yamllint enable rule:truthy

# Prevent concurrent updates to benchmark data
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Dynamic matrix generation based on available models
  setup-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      should_run: ${{ steps.check-changes.outputs.should_run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if benchmark should run
        id: check-changes
        run: |
          # Always run on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # For PRs, check if relevant files changed
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Check for changes in models, training, or benchmark files
            changed_files=$(git diff --name-only ${{ github.event.pull_request.base.sha }}...${{ github.sha }})
            pattern='(xtylearner/models/|xtylearner/training/|eval\.py|benchmark_config\.json|\.github/workflows/benchmark\.yml)'

            if echo "$changed_files" | grep -E "$pattern"; then
              echo "should_run=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi

      - name: Generate benchmark matrix
        id: set-matrix
        if: steps.check-changes.outputs.should_run == 'true'
        env:
          GITHUB_EVENT_NAME: ${{ github.event_name }}
          FULL_BENCHMARK: ${{ github.event.inputs.full_benchmark }}
        run: |
          python3 scripts/generate_benchmark_matrix.py --output "$GITHUB_OUTPUT"

  prepare-runtime:
    needs: setup-matrix
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python with cache
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml
            uv.lock

      - name: Restore pip and torch caches
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/torch
          key: ${{ runner.os }}-deps-${{ hashFiles('requirements.txt', 'pyproject.toml', 'uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-deps-

      - name: Restore prepared virtualenv
        id: venv-cache
        uses: actions/cache@v4
        with:
          path: .venv
          key: >-
            ${{ runner.os }}-benchmark-venv-${{ hashFiles(
              'requirements.txt',
              'pyproject.toml',
              'uv.lock',
              '.github/workflows/benchmark.yml'
            ) }}

      - name: Build virtualenv
        if: steps.venv-cache.outputs.cache-hit != 'true'
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
          pip install matplotlib pandas numpy scipy
          pip install -e .

      - name: Package prepared environment
        run: |
          tar -czf prepared-venv.tar.gz .venv

      - name: Upload prepared environment
        uses: actions/upload-artifact@v4
        with:
          name: prepared-venv
          path: prepared-venv.tar.gz
          retention-days: 3

  # Parallel benchmark execution
  benchmark:
    needs: [setup-matrix, prepare-runtime]
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.setup-matrix.outputs.matrix) }}
      max-parallel: 6  # Increased for more models

    steps:
      - uses: actions/checkout@v4

      - name: Download prepared environment
        uses: actions/download-artifact@v4
        with:
          name: prepared-venv
          path: .

      - name: Extract environment
        id: extract-env
        run: |
          SECONDS=0
          tar -xzf prepared-venv.tar.gz
          echo "seconds=$SECONDS" >> $GITHUB_OUTPUT

      - name: Ensure dataset cache directory
        run: mkdir -p .cache/xtylearner

      - name: Restore dataset cache
        uses: actions/cache@v4
        with:
          path: .cache/xtylearner
          key: ${{ runner.os }}-dataset-${{ matrix.dataset }}-${{ hashFiles('eval.py') }}
          restore-keys: |
            ${{ runner.os }}-dataset-${{ matrix.dataset }}-

      # Run benchmarks with retry logic
      - name: Run benchmarks
        id: run-benchmarks
        env:
          PYTHONUNBUFFERED: 1
        run: |
          set -euo pipefail
          SECONDS=0

          echo "Running benchmark for model: ${{ matrix.model }}, dataset: ${{ matrix.dataset }}"

          if ! timeout 1800 ./.venv/bin/python eval.py \
            --model ${{ matrix.model }} \
            --dataset ${{ matrix.dataset }} \
            --output results-${{ matrix.model }}-${{ matrix.dataset }}.json; then
            echo "Benchmark failed or timed out for ${{ matrix.model }} on ${{ matrix.dataset }}"
            ./.venv/bin/python scripts/write_timeout_result.py \
              --path "results-${{ matrix.model }}-${{ matrix.dataset }}.json" \
              --model "${{ matrix.model }}" \
              --dataset "${{ matrix.dataset }}"
          fi

          echo "seconds=$SECONDS" >> $GITHUB_OUTPUT

      - name: Persist timing metadata
        env:
          MODEL: ${{ matrix.model }}
          DATASET: ${{ matrix.dataset }}
          ENV_SETUP_SECONDS: ${{ steps.extract-env.outputs.seconds }}
          BENCHMARK_SECONDS: ${{ steps.run-benchmarks.outputs.seconds }}
        run: |
          ./.venv/bin/python scripts/update_timing_metadata.py \
            --model "$MODEL" \
            --dataset "$DATASET" \
            --env-seconds "${ENV_SETUP_SECONDS:-0}" \
            --benchmark-seconds "${BENCHMARK_SECONDS:-0}" \
            --results-path "results-${{ matrix.model }}-${{ matrix.dataset }}.json" \
            --output-path "timing-${{ matrix.model }}-${{ matrix.dataset }}.json"

      # Upload individual results
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.model }}-${{ matrix.dataset }}
          path: |
            results-${{ matrix.model }}-${{ matrix.dataset }}.json
            timing-${{ matrix.model }}-${{ matrix.dataset }}.json
          retention-days: 7

  # Aggregate and analyze results
  analyze:
    needs: [setup-matrix, benchmark]
    if: needs.setup-matrix.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      pages: write
      id-token: write

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download prepared environment
        uses: actions/download-artifact@v4
        with:
          name: prepared-venv
          path: .

      - name: Extract environment
        run: |
          tar -xzf prepared-venv.tar.gz

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: ./results

      - name: Aggregate results and update history
        run: |
          ./.venv/bin/python scripts/aggregate_results.py \
            --input-dir ./results \
            --history-file history.json \
            --output-file current-benchmarks.json

      - name: Generate benchmark charts and summary
        if: always()
        env:
          MPLBACKEND: Agg
        run: |
          ./.venv/bin/python scripts/generate_simple_charts.py \
            --history history.json \
            --output-dir ./charts \
            --markdown-output benchmark-summary.md

      - name: Summarize benchmark timings
        if: always()
        run: |
          ./.venv/bin/python scripts/summarize_timings.py \
            --current current-benchmarks.json \
            --timing-dir ./results \
            --summary-out timing-summary.md

      - name: Attach summaries to job summary
        if: always()
        run: |
          if [ -f benchmark-summary.md ]; then
            cat benchmark-summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No benchmark results available._" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -f timing-summary.md ]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            cat timing-summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No timing data captured._" >> "$GITHUB_STEP_SUMMARY"
          fi

      # Deploy to GitHub Pages (main branch only)
      - name: Deploy dashboard
        if: github.ref == 'refs/heads/main' && success()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./charts

      - name: Upload benchmark history
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-history
          path: history.json
          retention-days: 90
