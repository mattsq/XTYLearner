# yamllint disable rule:truthy
---
name: Benchmark Models

on:
  pull_request:
    types: [opened, synchronize]
  push:
    branches: [main]

# yamllint enable rule:truthy

jobs:
  detect-changes:
    if: github.actor != 'github-actions[bot]'
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.models.outputs.should_run }}
      model_chunks: ${{ steps.models.outputs.model_chunks }}
      models: ${{ steps.models.outputs.models }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}
      - name: Fetch base reference
        if: github.event_name == 'pull_request'
        run: git fetch origin ${{ github.base_ref }} --depth=1
      - name: Determine benchmark models
        id: models
        env:
          DEFAULT_MODELS: cycle_dual,mean_teacher
          CHUNK_SIZE: '1'
        run: |
          python <<'PY'
          import json
          import os
          import subprocess
          from pathlib import Path

          def has_ref(ref: str) -> bool:
              return subprocess.run(
                  ["git", "rev-parse", "--verify", ref],
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL,
                  check=False,
              ).returncode == 0

          base_ref = os.environ.get("GITHUB_BASE_REF")
          compare_ref = f"origin/{base_ref}" if base_ref else "origin/main"
          if not has_ref(compare_ref):
              compare_ref = "HEAD^"

          diff_args = ["git", "diff", "--name-only"]
          if has_ref(compare_ref):
              diff_args.append(f"{compare_ref}...HEAD")

          result = subprocess.run(
              diff_args,
              check=False,
              stdout=subprocess.PIPE,
              text=True,
          )

          changed_files: list[str] = []
          for line in result.stdout.splitlines():
              line = line.strip()
              if line:
                  changed_files.append(line)

          changed_models = set()
          for path in changed_files:
              if (
                  not path.startswith("xtylearner/models/")
                  or not path.endswith(".py")
              ):
                  continue
              name = Path(path).stem
              if name.endswith("_model"):
                  name = name[: -len("_model")]
              changed_models.add(name)

          default_models = [
              model.strip()
              for model in os.environ.get("DEFAULT_MODELS", "").split(",")
              if model.strip()
          ]
          if changed_models:
              selected = sorted(set(default_models) | changed_models)
          else:
              selected = default_models

          if not selected:
              selected = ["cycle_dual"]

          chunk_size = max(1, int(os.environ.get("CHUNK_SIZE", "1")))
          chunks: list[list[str]] = []
          for index in range(0, len(selected), chunk_size):
              chunks.append(selected[index : index + chunk_size])

          output_path = Path(os.environ["GITHUB_OUTPUT"])
          chunk_output = ["should_run=true", f"models={','.join(selected)}"]
          chunk_strings = [",".join(chunk) for chunk in chunks]
          chunk_output.append("model_chunks=" + json.dumps(chunk_strings))
          output_text = "\n".join(chunk_output) + "\n"
          output_path.write_text(output_text, encoding="utf-8")

          print("Selected models:", ", ".join(selected))
          for index, chunk in enumerate(chunks):
              print(f"Chunk {index}: {', '.join(chunk)}")
          PY

  benchmark:
    needs: detect-changes
    if: needs.detect-changes.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    env:
      PYTHON_VERSION: '3.12'
    strategy:
      fail-fast: false
      matrix:
        model_chunk: >-
          ${{ fromJson(needs.detect-changes.outputs.model_chunks) }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}
      - name: Ensure main branch is available
        run: |
          set -euo pipefail
          git fetch origin main --depth=50
          current_branch=$(git rev-parse --abbrev-ref HEAD)
          if [ "$current_branch" = "main" ]; then
            exit 0
          fi
          if git show-ref --verify --quiet refs/heads/main; then
            git branch --force main origin/main
          else
            git branch main origin/main
          fi
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'  # yamllint disable-line rule:truthy
          cache-dependency-path: |
            asv.conf.json
            requirements.txt
      - name: Install ASV tooling
        run: |
          python -m pip install --upgrade pip
          python -m pip install asv
      - name: Compute ASV cache key
        id: asv-cache
        run: |
          python <<'PY'
          import hashlib
          import os
          from pathlib import Path

          paths = [Path('asv.conf.json'), Path('requirements.txt')]
          digest = hashlib.sha256()
          for path in paths:
              if path.exists():
                  digest.update(path.read_bytes())

          fingerprint = digest.hexdigest()
          prefix = 'asv-env-ubuntu-latest-py3.12'

          output = Path(os.environ['GITHUB_OUTPUT'])
          with output.open('a', encoding='utf-8') as handle:
              handle.write(f"fingerprint={fingerprint}\n")
              handle.write(f"prefix={prefix}\n")
              handle.write(f"cache_key={prefix}-{fingerprint}\n")
          PY
      - name: Restore cached ASV environments
        id: cache-asv
        uses: actions/cache@v4
        with:
          path: .asv/env
          key: ${{ steps.asv-cache.outputs.cache_key }}
          restore-keys: |
            ${{ steps.asv-cache.outputs.prefix }}-
      - name: Run ASV benchmarks
        env:
          BENCHMARK_MODELS: ${{ matrix.model_chunk }}
        run: |
          set -euo pipefail
          
          # Create unique machine name for this chunk
          CHUNK_ID=$(echo "${{ matrix.model_chunk }}" | tr ' ,/' '-' | tr -cd '[:alnum:]-')
          MACHINE_NAME="github-actions-${CHUNK_ID}"
          
          echo "Using machine name: $MACHINE_NAME"
          echo "Model chunk: ${BENCHMARK_MODELS:-<default>}"
          
          OS_NAME=$(python - <<'PY'
          import platform

          system = platform.system() or "UnknownOS"
          release = platform.release() or ""
          print((system + " " + release).strip())
          PY
          )

          ARCH=$(uname -m)

          CPU_INFO=$(python - <<'PY'
          import platform
          import subprocess

          cpu = platform.processor()
          if not cpu or cpu.lower() in {"", "x86_64", "amd64"}:
              try:
                  output = subprocess.check_output(["lscpu"], text=True)
              except Exception:
                  cpu = "Unknown CPU"
              else:
                  model_line = None
                  for line in output.splitlines():
                      if line.lower().startswith("model name:"):
                          model_line = line
                          break
                  if model_line:
                      cpu = model_line.split(":", 1)[1].strip()
                  else:
                      cpu = (
                          output.strip().splitlines()[0].strip()
                          or "Unknown CPU"
                      )
          print(cpu)
          PY
          )

          NUM_CPU=$(python - <<'PY'
          import os

          print(os.cpu_count() or 1)
          PY
          )

          RAM=$(python - <<'PY'
          from pathlib import Path

          meminfo = Path("/proc/meminfo")
          total = None
          if meminfo.exists():
              for line in meminfo.read_text().splitlines():
                  if line.startswith("MemTotal:"):
                      parts = line.split()
                      if len(parts) >= 2:
                          try:
                              total = int(parts[1]) * 1024
                          except ValueError:
                              total = None
                      break

          if total:
              gib = total / (1024 ** 3)
              if gib >= 1:
                  value = f"{gib:.1f}GB"
              else:
                  mib = total / (1024 ** 2)
                  value = f"{mib:.0f}MB"
          else:
              value = "Unknown RAM"
          print(value)
          PY
          )

          asv machine \
            --machine "$MACHINE_NAME" \
            --os "$OS_NAME" \
            --arch "$ARCH" \
            --cpu "$CPU_INFO" \
            --num_cpu "$NUM_CPU" \
            --ram "$RAM" \
            --yes
          COMMIT_HASH=$(git rev-parse HEAD)
          if git rev-parse "${COMMIT_HASH}^" >/dev/null 2>&1; then
            COMMIT_SPEC="${COMMIT_HASH}^!"
          else
            COMMIT_SPEC="$COMMIT_HASH"
          fi
          echo "Running benchmarks for commit: $COMMIT_HASH"
          asv run "$COMMIT_SPEC" \
            --machine "$MACHINE_NAME" \
            --config asv.conf.json \
            --show-stderr
          if ! find .asv/results/"$MACHINE_NAME" -maxdepth 1 \
              -name "*.json" -not -name "machine.json" -print -quit; then
            echo "No benchmark result files were produced"
            exit 1
          fi
          asv publish --config asv.conf.json --html-dir .asv/html
      - name: Prepare artifact name
        id: artifact
        env:
          MODEL_CHUNK: ${{ matrix.model_chunk }}
        run: |
          SAFE_NAME=${MODEL_CHUNK:-default}
          SAFE_NAME=$(printf '%s' "$SAFE_NAME" | tr ' ,/' '---')
          SAFE_NAME=$(printf '%s' "$SAFE_NAME" | tr -c 'A-Za-z0-9-_' '_')
          echo "name=asv-results-$SAFE_NAME" >> "$GITHUB_OUTPUT"
      - name: Upload benchmark artifacts
        env:
          MODEL_CHUNK: ${{ matrix.model_chunk }}
        run: |
          # Get machine name for this chunk
          CHUNK_ID=$(echo "${{ matrix.model_chunk }}" | tr ' ,/' '-' | tr -cd '[:alnum:]-')
          MACHINE_NAME="github-actions-${CHUNK_ID}"
          echo "machine_name=$MACHINE_NAME" >> "$GITHUB_OUTPUT"
        id: machine-name
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.artifact.outputs.name }}
          path: |
            .asv/results/${{ steps.machine-name.outputs.machine_name }}
            .asv/html

  consolidate-results:
    needs: [detect-changes, benchmark]
    if: needs.detect-changes.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}
      - name: Ensure main branch exists
        run: |
          # Ensure we have the main branch for ASV publish
          if ! git show-ref --verify --quiet refs/heads/main; then
            if git show-ref --verify --quiet refs/remotes/origin/main; then
              git branch main origin/main
            else
              git fetch origin main:main
            fi
          fi
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: |
            asv.conf.json
            requirements.txt
      - name: Install ASV tooling
        run: |
          python -m pip install --upgrade pip
          python -m pip install asv
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: asv-results-*
          path: artifact-downloads
          merge-multiple: false
      - name: Consolidate benchmark results
        run: |
          set -euo pipefail
          echo "=== Consolidating benchmark results ==="

          # Create results directory
          mkdir -p .asv/results

          # Copy all machine results from artifacts (using unique machine names)
          echo "=== Copying machine results ==="
          machine_count=0
          for artifact_dir in artifact-downloads/asv-results-*/results/*; do
            if [ -d "$artifact_dir" ]; then
              machine_name=$(basename "$artifact_dir")
              echo "Copying results for machine: $machine_name"
              cp -r "$artifact_dir" ".asv/results/$machine_name"
              machine_count=$((machine_count + 1))
            fi
          done

          echo "Consolidated $machine_count machines:"
          ls -la .asv/results/

          # Validate we have results
          if [ "$machine_count" -eq 0 ]; then
            echo "ERROR: No machine results found to consolidate"
            exit 1
          fi

          # Generate unified HTML (ASV will handle multiple machines correctly)
          echo "=== Generating unified HTML ==="
          echo "Current git branches:"
          git branch -a || echo "Git branch listing failed"
          echo "Current commit:"
          git rev-parse HEAD || echo "Git rev-parse failed"

          # Create a temporary config that doesn't require specific branches
          cp asv.conf.json asv-publish.conf.json
          # Get current branch name
          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
          echo "Current branch: $CURRENT_BRANCH"

          # Update config to use current branch instead of main
          jq --arg branch "$CURRENT_BRANCH" '.branches = [$branch]' asv.conf.json > asv-publish.conf.json

          # Try publish with modified config
          echo "=== ASV Publish Debug ==="
          echo "Config file contents:"
          cat asv-publish.conf.json
          echo "Available result files:"
          find .asv/results -name "*.json" | head -10

          # ASV publish will handle multiple machines correctly
          asv publish --config asv-publish.conf.json --html-dir .asv/html --no-pull --verbose || {
            echo "ASV publish failed, trying fallback approach..."

            # Fallback: create minimal working HTML
            echo "Generating minimal HTML manually..."
            mkdir -p .asv/html
            cat > .asv/html/index.html << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>XTYLearner Benchmarks</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
        .benchmark { border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }
        .machine { background: #f5f5f5; margin: 10px 0; padding: 10px; border-radius: 3px; }
    </style>
</head>
<body>
    <h1>XTYLearner Benchmark Results</h1>
    <p>‚úÖ Benchmark execution completed successfully</p>
    <p>‚úÖ Results collected from parallel execution</p>
    
    <div class="benchmark">
        <h3>Execution Summary</h3>
EOF
            echo "        <p>Machines: $machine_count</p>" >> .asv/html/index.html
            echo "        <p>Models: ${{ needs.detect-changes.outputs.models }}</p>" >> .asv/html/index.html
            cat >> .asv/html/index.html << 'EOF'
        <p>Commit: $(git rev-parse --short HEAD)</p>
    </div>
    
    <div class="benchmark">
        <h3>Available Machines</h3>
EOF
            for machine_dir in .asv/results/*; do
              if [ -d "$machine_dir" ]; then
                machine_name=$(basename "$machine_dir")
                echo "        <div class=\"machine\">$machine_name</div>" >> .asv/html/index.html
              fi
            done
            cat >> .asv/html/index.html << 'EOF'
    </div>
</body>
</html>
EOF
          }

          # Validate HTML generation
          if [ ! -f ".asv/html/index.html" ]; then
            echo "ERROR: HTML generation completely failed"
            exit 1
          fi

          echo "HTML generation completed"
          echo "Generated files:"
          find .asv/html -name "*.html" | head -10
          echo "HTML file size:"
          ls -la .asv/html/index.html
      - name: Upload consolidated artifacts
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-asv-results
          path: |
            .asv/results
            .asv/html

  deploy-to-pages:
    needs: [detect-changes, consolidate-results]
    if: needs.detect-changes.outputs.should_run == 'true' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    # Note: Remove environment block to avoid protection rule issues
    # environment:
    #   name: github-pages
    #   url: ${{ steps.deployment.outputs.page_url }}
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: Download consolidated results
        uses: actions/download-artifact@v4
        with:
          name: consolidated-asv-results
          path: asv-consolidated
      - name: Validate deployment package
        id: validate
        run: |
          set -euo pipefail
          echo "=== Validating deployment package ==="

          # Debug: Show actual artifact structure
          echo "=== Artifact structure ==="
          find asv-consolidated -type f | head -20
          echo "=== Looking for HTML files ==="
          find asv-consolidated -name "*.html" | head -10

          # Find the actual HTML directory structure in artifact
          html_dir=""

          # Try different possible locations
          possible_dirs=(
            "asv-consolidated/.asv/html"
            "asv-consolidated/html"
            "asv-consolidated/.asv/html-test"
          )

          for dir in "${possible_dirs[@]}"; do
            if [ -d "$dir" ] && [ -f "$dir/index.html" ]; then
              html_dir="$dir"
              break
            fi
          done

          # If no standard directory found, search for any index.html
          if [ -z "$html_dir" ]; then
            echo "Standard HTML directories not found, searching for index.html..."
            index_file=$(find asv-consolidated -name "index.html" -type f | head -1)
            if [ -n "$index_file" ]; then
              html_dir=$(dirname "$index_file")
              echo "Found index.html at: $index_file"
            fi
          fi

          if [ -z "$html_dir" ]; then
            echo "ERROR: Cannot find HTML directory in artifact"
            echo "Available directories:"
            find asv-consolidated -type d
            echo "Available HTML files:"
            find asv-consolidated -name "*.html" -type f
            exit 1
          fi

          echo "Found HTML directory: $html_dir"

          # Normalize to a standard location for upload
          normalized_dir="pages-content"
          echo "Normalizing HTML content to: $normalized_dir"

          # Copy HTML content to normalized location
          cp -r "$html_dir" "$normalized_dir"

          # Output the normalized path for the upload step
          echo "upload_path=$normalized_dir" >> "$GITHUB_OUTPUT"

          # Check required files exist
          if [ ! -f "$normalized_dir/index.html" ]; then
            echo "ERROR: Missing index.html in $normalized_dir"
            echo "Contents of normalized directory:"
            ls -la "$normalized_dir/" || echo "Directory listing failed"
            exit 1
          fi

          # Check for required ASV assets (optional - may not exist in fallback HTML)
          required_files=("asv.js" "asv.css" "asv_ui.js")
          missing_assets=0
          for file in "${required_files[@]}"; do
            if [ ! -f "$normalized_dir/$file" ]; then
              echo "WARNING: Missing ASV file: $file (may be fallback HTML)"
              missing_assets=$((missing_assets + 1))
            fi
          done

          # Validate index.html doesn't contain error messages (but allow fallback)
          if grep -qi "error\|no.*data.*available" "$normalized_dir/index.html"; then
            echo "WARNING: HTML contains error indicators (may be fallback)"
            echo "First few lines of HTML:"
            head -10 "$normalized_dir/index.html"
          fi

          echo "Deployment package validation completed"
          echo "Original HTML directory: $html_dir"
          echo "Normalized upload path: $normalized_dir"
          echo "Missing ASV assets: $missing_assets/3"
          echo "Package contents:"
          find "$normalized_dir" -type f | head -20
      - name: Setup Pages
        uses: actions/configure-pages@v4
      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ${{ steps.validate.outputs.upload_path }}
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
      - name: Verify deployment
        if: always()
        run: |
          echo "=== Deployment Summary ==="
          echo "Deployment status: ${{ steps.deployment.outcome }}"
          if [ "${{ steps.deployment.outcome }}" = "success" ]; then
            echo "‚úÖ ASV benchmarks successfully deployed to GitHub Pages"
            echo "üîó URL: https://mattsq.github.io/XTYLearner/"
          else
            echo "‚ùå Deployment failed"
            echo "Check the deployment logs above for details"
          fi
